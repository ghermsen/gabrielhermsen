<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Gabriel Hermsen Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/gabrielhermsen/assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
							<div class="content" w3-include-html="/gabrielhermsen/components/header_python.html">
							</div>

							<section>

								<span><a href="https://www.linkedin.com/in/gabrielhermsen/" target="_blank"><img src="https://img.shields.io/badge/author-ghermsen-red.svg"/></a></span>
								<span ><a href="https://docs.python.org/3.9/" target="_blank"><img src="https://img.shields.io/badge/python-3.9-blue.svg"></a></span>
								<span ><a href="http://perso.crans.org/besson/LICENSE.html" target="_blank"><img src="https://img.shields.io/badge/License-GPLv3-blue.svg"></a></span>
								<span ><a href="https://github.com/ghermsen/airbnb_munchen/issues" target="_blank"><img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat"></a></span>

								<header class="main">
									<h1>Rossmann Stores Sale Prediction</h1>
								</header>

								<ul class="actions">
									<li><a href="https://github.com/ghermsen/rossmann_prediction/blob/main/rossmann_prediction_project.ipynb" target="_blank" class="button icon solid fa-code">Project's Code</a></li>
								</ul>

								<p>Dirk Rossmann GmbH, commonly known as Rossmann, is the largest drugstore chain in Germany in the number of stores, with 2196 
								stores in German territory (2020, <a href="https://www.statista.com/statistics/505614/number-of-drugstore-branches-germany" target="_blank">Statista)</a>. 
								In 2015, Rossmann launched a competition on <a href="https://www.kaggle.com/c/rossmann-store-sales/overview" target="_blank">Kaggle</a> 
								to forecast sales in stores for the next six weeks.</p>
								<p>This project will cover all the steps of a Data Science project from understanding the problem until the deployment of a solution 
								as seen above, and it will predict the sale of a particular store in the next six weeks.</p>
								<span class="image main" style="text-align: center"><img src="https://camo.githubusercontent.com/7723bec75c8df660d5dbcd75617b3f2dd8d9461682cb15d6cb352338e523b0ec/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f392f39352f526f73736d616e6e5f536368726966747a75675f6d69745f43656e746175722e6a7067"/><em>Photo by: <a href="https://commons.wikimedia.org/wiki/File:Rossmann_Schriftzug_mit_Centaur.jpg" target="_blank">Anakin81</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>, via Wikimedia Commons</em></span>

								<hr class="major" />

								<h2><a name="table_of_contents">Table of Contents</a></h2>
								<ul>
									<li><a href="#project_methodology">Project Methodology</a></li>
									<li><a href="#01_the_problem_and_the_solution">01. The Problem and the Solution</a></li>
									<li><a href="#02_data_understanding_and_data_preparation">02. Data Understanding and Data Preparation</a></li>
									<li><a href="#03_feature_engineering_and_feature_filtering">03. Feature Engineering and Feature Filtering</a></li>
									<li><a href="#04_exploratory_data_analysis">04. Exploratory Data Analysis</a></li>
									<li><a href="#05_data_preprocessing">05. Data Preprocessing</a></li>
									<li><a href="#06_feature_selection">06. Feature Selection</a></li>
									<li><a href="#07_machine_learning_modeling">07. Machine Learning Modeling</a></li>
									<li><a href="#08_hyperparameter_tuning">08. Hyperparameter Tuning</a></li>
									<li><a href="#09_understaing_the_error_and_business_performance">09. Understanding the Error and Business Performance</a></li>
									<li><a href="#10_model_deployment_and_final_solution">10. Final Model Deployment</a></li>
									<li><a href="#conclusion">Conclusion</a></li>
								</ul>

								<hr class="major" />
								
								<h2 id="title"><a name="project_methodology">Project Methodology</a></h2>
								<div class="subtitle">
									<a href="#01_the_problem_and_the_solution">(next section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>The methodology used in this project is the <strong>CRISP-DM</strong> (Cross-Industry Standard Process for Data Mining), 
								one of the most used methodologies in data science projects. In my opinion, the main advantages of using this methodology in 
								data science projects (when compared to others SEMMA and KDD) are that any industry can use CRISP-DM on its projects and the 
								agility in the generation of value.</p>
								<p>This methodology is divided in six main phases, <strong>business understanding, data understanding, data preparation, 
								modeling, evaluation and deployment</strong>, and when the six phases are completed a cycle of the CRISP-DM is done.</p>
								<p>For a better understanding, we will assume the following situation: we are in the modeling phase of the project, and 
								from our experience, it can be seen that the results can be better if a specific variable were created and implemented. 
								However, the data preparation phase has already been completed, which makes the implementation of this new variable only 
								happen in the second cycle of CRISP-DM. This method concludes the current cycle in less time and delivers value and knowledge 
								at the end of each phase, automatically generating a list of future implementations for future cycles.</p>
								<p>Below there is a diagram that shows this methodology:</p>
								<img width="30%" src="https://camo.githubusercontent.com/16e18b1a03d19b87fca12821cde6598640a72db29a1ec4683758b45f212a3920/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f622f62392f43524953502d444d5f50726f636573735f4469616772616d2e706e67" style="display:block; margin-left: auto; margin-right: auto;"><span class="image main" style="text-align: center"><em>Photo by: <a href="https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png" target="_blank">Kenneth Jensen</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0/" target="_blank">CC BY-SA 4.0</a>, via Wikimedia Commons</em><br></span>

								<hr class="major" />

								<h2 id="title"><a name="01_the_problem_and_the_solution">01. The Problem and the Solution</a></h2>
								<div class="subtitle">
									<a href="#02_data_understanding_and_data_preparation">(next section)</a> |
									<a href="#project_methodology">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>Here there is an understanding of the problem, and consequently, it is expected a delivery of value with a suggested solution. 
								Analyzing the description of the competition in <a href="https://www.kaggle.com/c/rossmann-store-sales" targe="_blank">Kaggle</a>, 
								the following can be observed: <em>"With thousands of individual managers predicting sales based on their unique circumstances, the 
								accuracy of results can be quite varied."</em>. With this mention, it seems that the problem lies in obtaining sales results with 
								varying accuracy by these thousands of managers.</p>
								<p>Thus, the project's objective is to create a machine learning model that can understand the factors that influence the sales of 
								each store and generate forecasts following these factors.</p>

								<hr class="major" />

								<h2 id="title"><a name="02_data_understanding_and_data_preparation">02. Data Understanding and Data Preparation</a></h2>
								<div class="subtitle">
									<a href="#03_feature_engineering_and_feature_filtering">(next section)</a> |
									<a href="#01_the_problem_and_the_solution">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								
								<h3>02.01 - Obtaining the Data</h3>
								<p>All data used in this project is on the website of the <a href="https://www.kaggle.com/c/rossmann-store-sales/data" target="_blank">Kaggle</a>.</p>
								<p>For this project, the following files were downloaded:</p>
								<ul>
									<li>Downloaded Files:</li>
									<ul>
										<li><em>train.csv</em> - historical data including Sales</li>
										<li><em>test.csv</em> - historical data excluding Sales</li>
										<li><em>store.csv</em> - supplemental information about the stores</li>
									</ul>
									<li>Github Repository</li>
									<ul>
										<li><em>train.csv</em> - <a href="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/data/train.csv">repository path</a></li>
										<li><em>test.csv</em> - <a href="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/data/test.csv">repository path</a></li>
										<li><em>store.csv</em> - <a href="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/data/store.csv">repository path</a></li>
									</ul>
								</ul>
								<p>After understanding the data, a dataset <code>df_raw</code> was created merging the datasets train.csv and store.csv with 
								the following code:</p>
								<pre><code>
# merging datasets

df_raw = pd.merge(df_sales_raw, df_store_raw, how = 'left', on = 'Store')
								</code></pre>

								<h3>02.02 - Data Dimensions and Type of Variables</h3>
								<p>Below, it is possible to see that our dataset has <strong>1,017,209 rows and 18 columns</strong> as well as the type of its variables:</p>
								<pre><code>
# dataset's size

print(f'Number of rows:\t\t {df1.shape[0]}')
print(f'Number of columns:\t {df1.shape[1]}')

    Number of rows:    1017209
    Number of columns:      18
    
# type of variables

df1.dtypes

    store                             int64
    day_of_week                       int64
    date                             object
    sales                             int64
    customers                         int64
    open                              int64
    promo                             int64
    state_holiday                    object
    school_holiday                    int64
    store_type                       object
    assortment                       object
    competition_distance            float64
    competition_open_since_month    float64
    competition_open_since_year     float64
    promo2                            int64
    promo2_since_week               float64
    promo2_since_year               float64
    promo_interval                   object
    
    dtype: object
								</code></pre>
								<p>It can be observed that the variable <code>date</code> is an object. As this variable represent time, this variable passed for a 
								process of transformation to <code>datetime64[ns]</code>.</p>
								<pre><code>
# transforming date to datetime

df1['date'] = pd.to_datetime(df1['date'])
df1.dtypes

    store                                    int64
    day_of_week                              int64
    date                            datetime64[ns]
    sales                                    int64
    customers                                int64
    open                                     int64
    promo                                    int64
    state_holiday                           object
    school_holiday                           int64
    store_type                              object
    assortment                              object
    competition_distance                   float64
    competition_open_since_month           float64
    competition_open_since_year            float64
    promo2                                   int64
    promo2_since_week                      float64
    promo2_since_year                      float64
    promo_interval                          object
    dtype: object
								</code></pre>

								<h3>02.03 - Checking NA's and Filling Out NA's</h3>
								<p>The quality of a dataset is directly related to the number of missing values. It is important to understand early on whether 
								these null values are significant concerning the total number of entries to avoid future problems in our analysis. Below, it is 
								possible to see that the following variables present missing data:</p>
								<ul>
									<li><code>competition_distance</code> = 2,642 or <strong>00.26%</strong></li>
									<li><code>competition_open_since_month</code> = 323,348 or <strong>31.79%</strong></li>
									<li><code>competition_open_since_year</code> = 323,348 or <strong>31.79%</strong></li>
									<li><code>promo2_since_year</code> = 508,031 or <strong>49.94%</strong></li>
									<li><code>promo2_since_week</code> = 508,031 or <strong>49.94%</strong></li>
									<li><code>promo_interval</code> = 508,031 or <strong>49.94%</strong></li>
								</ul>
								<pre><code>
# checking na's

df1.isna().sum().sort_values()

    store                                0
    promo2                               0
    assortment                           0
    store_type                           0
    state_holiday                        0
    promo                                0
    school_holiday                       0
    customers                            0
    sales                                0
    date                                 0
    day_of_week                          0
    open                                 0
    competition_distance              2642
    competition_open_since_month    323348
    competition_open_since_year     323348
    promo2_since_year               508031
    promo2_since_week               508031
    promo_interval                  508031
    
    dtype: int64
								</code></pre>
								<p>Above, it is seen that the variable <code>competition_distance</code> shows missing values that are not significant for the 
								analysis. These missing values could be deleted as there would be no significant problems in developing the project. However, 
								thinking about the business, the dictionary of variables shows that this variable represents the <strong>distance in meters to 
								the nearest competitor store</strong>. In this way, we can fill these values with a value much greater than the maximum value of 
								the largest competition in existence. For this, we will consider 2x the maximum value of the competition.</p>
								<pre><code>
# filling out competition_distance

max_value = df1['competition_distance'].max()

df1['competition_distance'] = df1['competition_distance'].apply(lambda x: max_value * 2 if math.isnan(x) else x)
								</code></pre>
								<p>The variables <code>competition_open_since_month</code> and <code>competition_open_since_year</code> have similar logic. Analyzing 
								the variable's description in the variable dictionary, we see that <strong>they gives the approximate year and month of the time 
								the nearest competitor was opened</strong>. There are two reasons why this variable has missing values, or this store does not have 
								a close competitor, so there is no opening date for this competition, and the other reason is that there is a competitor. However, 
								we do not know when this competition started, and to replace these NA's, the following assumption will be made. If the value is NA, 
								the date of the respective sale (in year and month) will be copied to replace, since from that month and year it can be filtered in 
								future dates if there is a competitor and this affected the sales of a particular store.</p>
								<pre><code>
# filling out competition_open_since_month

df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) 
                                                else x['competition_open_since_month'], axis = 1)

# filling out competition_open_since_year 

df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) 
                                               else x['competition_open_since_year'], axis = 1)
								</code></pre>
								<p>The variables <code>promo2_since_year</code> and <code>promo2_since_week</code> have similar logic. Analyzing the variable's 
								description in the variable dictionary, we see that <strong>they describes the year and calendar week when the store started 
								participating in Promo2</strong>, where Promo2 is a continuation of a promotion that some stores have joined where zero represents 
								that the store is not participating. One means that the store is participating. If the value shows NA, it shows that the store is not 
								participating in Promo2. For this, the same logic of filling NA values in the variables <code>competition_open_since_month</code> and 
								<code>competition_open_since_year</code> will be applied.</p>
								<pre><code>
# filling out promo2_since_week  

df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) 
                                     else x['promo2_since_week'], axis = 1)

# filling out promo2_since_year

df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) 
                                     else x['promo2_since_year'], axis = 1)
								</code></pre>
								<p>The variable <code>promo_interval</code> <strong>describes the consecutive intervals Promo2 is started, naming the months the 
								promotion is started anew. E.g. "Feb,May,Aug,Nov" means each round starts in February, May, August, November of any given year for that 
								store</strong>. This interval shows which months Promo2 was active in the store. To replace the NA's, a dictionary mapping the months 
								will be created. After this process, all the NA's values of the variable <code>promo_interval</code> will be replaced by zero. After 
								this process, a new column called <code>month_map</code> is created, and it will receive the value of the month that the sale was 
								carried out being mapped by the created dictionary. To conclude this process, a new column called <code>is_promo</code> is created to 
								receive values zero and one, where zero means that the sale was made outside the promotion interval and one that the sale was made 
								during the store's promotion interval.</p>
								<pre><code>
# filling out promo_interval           

# creating the dictionary month_map
month_map = {1:'Jan',
             2:'Feb',
             3:'Mar',
             4:'Apr',
             5:'Mai',
             6:'Jun',
             7:'Jul',
             8:'Aug',
             9:'Sep',
             10:'Oct',
             11:'Nov',
             12:'Dec'}

# filling out NA's with 0 in promo_interval
df1['promo_interval'].fillna(0, inplace = True)

# mapping the name of the month
df1['month_map'] = df1['date'].dt.month.map(month_map)

# creating a column to identify if in this date the store was in the promo interval or not
df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 
                                                             else 1 if x['month_map'] in x['promo_interval'].split(',') 
                                                             else 0, axis = 1)
								</code></pre>
								<p>After these processes, all the NA's were filled out.</p>
								<pre><code>
df1.isna().sum()

    store                           0
    day_of_week                     0
    date                            0
    sales                           0
    customers                       0
    open                            0
    promo                           0
    state_holiday                   0
    school_holiday                  0
    store_type                      0
    assortment                      0
    competition_distance            0
    competition_open_since_month    0
    competition_open_since_year     0
    promo2                          0
    promo2_since_week               0
    promo2_since_year               0
    promo_interval                  0
    month_map                       0
    is_promo                        0
    
    dtype: int64
    
# changing variables type

df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)

df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)

df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)

df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)
								</code></pre>

								<h3>02.04 - Descriptive Statistics</h3>
								<p>Descriptive statistics will provide knowledge of the business and enable the detection of some errors. Descriptive statistics is 
								composed of two types of metrics, the metrics of central tendency and dispersion.</p>

								<h4>02.04.01 - Numerical Attributes</h4>
								<p>The descriptive statistics shows that the average sales is <strong>5,773.82 €</strong>, very close to the median of 
								<strong>5,744.00 €</strong>. We can also see that the average number of customers in stores is <strong>633.14</strong> with a standard 
								deviation of <strong>464</strong>. In other words, there are days that the stores can receive more than 1000 customers and days that 
								stores receive less than 200 customers.</p>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/02.04.01_desc_stat_num_att.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>02.04.02 - Categorical Attributes</h4>
								<p>One of the most practical ways to do descriptive statistics for categorical variables is using the <strong>boxplot graph</strong>. 
								This graph shows us the maximum and minimum values, represented by the upper and lower horizontal lines. The first and third quartiles 
								are represented by the upper and lower lines from the boxes and the median, the central line in each box.</p>
								<p>There is the possibility of identifying outliers with these graphs, represented by the points above or below the lines representing 
								the maximum and minimum.</p>
								<p>Once plotted, we can see the relationship between categorical variables and sales. We can see that the median of the 
								<code>state_holiday</code> type b (Easter Holiday) and type c (Christmas) are greater than type a (Public Holiday). The 
								<code>store_type</code> b has the highest median followed by c, and stores with <code>assortment</code> b (extra) have the highest median.</p>
								<img width="80%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/02.04.02_box_plot_cat_att.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<hr class="major" />

								<h2 id="title"><a name="03_feature_engineering_and_feature_filtering">03. Feature Engineering and Feature Filtering</a></h2>
								<div class="subtitle">
									<a href="#04_exploratory_data_analysis">(next section)</a> |
									<a href="#02_data_understanding_and_data_preparation">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>In this stage of the project, variables will be created from existing variables. For this, a hypothesis map was generated to assist 
								which variables must be created in order to accept the hypotheses.</p>

								<h3>03.01 - Hypothesis Map and Hyphothesis Creation</h3>
								<p>The Hypothesis Map was developed following five main factors that could influence the daily sales: <strong>customers characteristics, 
								products characteristics, location characteristics, stores characteristics, and time characteristics</strong>.</p>
								<img width="50%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/mental_map.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<p>Based on these characteristics and the data available on the dataset, the following list of hypotheses were developed:</p>
								<ul>
									<li>1- Stores with a larger assortment should sell more.</li>
									<li>2- Stores with closer competitors should sell less.</li>
									<li>3- Stores with longer competitors should sell more.</li>
									<li>4- Stores with active promotions for longer should sell more.</li>
									<li>5- Stores with more days of promotion should sell more.</li>
									<li>6- Stores with more consecutive promotions should sell more.</li>
									<li>7- Stores open during the Christmas holiday should sell more.</li>
									<li>8- Stores should sell more over the years.</li>
									<li>9- Stores should sell more in the second half of the year.</li>
									<li>10- Stores should sell more after the 10th of each month.</li>
									<li>11- Stores should sell less on weekends.</li>
									<li>12- Stores should sell less during school holidays.</li>
								</ul>

								<h3>03.02 - Feature Engineering</h3>
								<p>According with the final hypothesis list, the following variables were created or modified:</p>
								<div class="row">
									<div class="col-6 col-12-small">
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Variable</th>
														<th>Description</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td><code>year</code></td>
														<td>year of the observation</td>
													</tr>
													<tr>
														<td><code>month</code></td>
														<td>month of the observation</td>
													</tr>
													<tr>
														<td><code>day</code></td>
														<td>day of the observation</td>
													</tr>
													<tr>
														<td><code>week_of_year</code></td>
														<td>week of the year of the observation</td>
													</tr>
													<tr>
														<td><code>year_week</code></td>
														<td>year and week of the observation</td>
													</tr>
													<tr>
														<td><code>competition_since</code></td>
														<td>date that started a competition close to the store</td>
													</tr>
													<tr>
														<td><code>competition_time_month</code></td>
														<td>period in months that there is this competition close to the store</td>
													</tr>
													<tr>
														<td><code>promo_since</code></td>
														<td>date that the store joined a promotion</td>
													</tr>
													<tr>
														<td><code>promo_time_week</code></td>
														<td>period in weeks that the store joined a promotion</td>
													</tr>
													<tr>
														<td><code>assortment</code></td>
														<td>modification of the variable with the correct assortment</td>
													</tr>
													<tr>
														<td><code>state_holiday</code></td>
														<td>modification of the variable with the correct holiday name</td>
													</tr>
												</tbody>
											</table>
										</div>
									</div>
									<div class="col-6 col-12-small">
									</div>
								</div>
								<pre><code>
# year

df2['year'] = df2['date'].dt.year

# month

df2['month'] = df2['date'].dt.month

# day

df2['day'] = df2['date'].dt.day

# week_of_year

df2['week_of_year'] = df2['date'].dt.weekofyear

# year_week

df2['year_week'] = df2['date'].dt.strftime('%Y-%W')

# competition_since

df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year = x['competition_open_since_year'], month = x['competition_open_since_month'], day = 1), axis = 1)
df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)

# promo_since

df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)
df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days = 7))
df2['promo_time_week'] = ((df2['date'] - df2['promo_since']) / 7).apply(lambda x: x.days).astype(int)

# assortment

df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')

# state_holiday

df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')
								</code></pre>

								<h3>03.03 - Feature Filtering</h3>
								<p>After the feature engineering, it is necessary to filter the dataset before starting the Exploratory Data Analysis and creating 
								the model.</p>
								<p>This project has the objective to predict the total sales in the next six weeks. If we have a close looking in the dataset, there 
								are days that the stores are closed, and consequently, the sales are equal to zero. This information is not relevant to the model 
								because of this is conducted filtering on these rows.</p>
								<pre><code>
df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]
								</code></pre>
								<p>After this process, it is possible to see some variables that will not be available during the prediction, as <code>clients</code>. 
								This variable shows the number of clients in the store on that day. For this, it would be necessary another model predict the number 
								of clients and send the output to the prediction model. Because of this, this variable is deleted from the dataset. I will also drop 
								other variables as open (already filtered) and <code>promo_interval</code>, and <code>month_map</code> (used to generate other variables) 
								from the dataset.</p>
								<p>Below, the code made for this process and the variables selected to continue the project.</p>
								<pre><code>
cols_drop = ['customers', 'open', 'promo_interval', 'month_map']

df3 = df3.drop(cols_drop, axis = 1)

df3.columns

    Index(['store', 'day_of_week', 'date', 'sales', 'promo', 'state_holiday',
           'school_holiday', 'store_type', 'assortment', 'competition_distance',
           'competition_open_since_month', 'competition_open_since_year', 'promo2',
           'promo2_since_week', 'promo2_since_year', 'is_promo', 'year', 'month',
           'day', 'week_of_year', 'year_week', 'competition_since',
           'competition_time_month', 'promo_since', 'promo_time_week'],
          dtype='object')
								</code></pre>

								<hr class="major" />

								<h2 id="title"><a name="04_exploratory_data_analysis">04. Exploratory Data Analysis</a></h2>
								<div class="subtitle">
									<a href="#05_data_preprocessing">(next section)</a> |
									<a href="#03_feature_engineering_and_feature_filtering">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>This step of the project aims to determine how the predictor variables impact the target variable (<code>sales</code>) and how much 
								this impact is. In this step, it is conducted a hypotheses validation and how relevant these hypotheses are.</p>
								<p>This step is divided into three main parts:</p>
								<ul>
									<li><strong>Univariate Analysis</strong> - an overview of the target variable and the numerical and categorical variables.</li>
									<li><strong>Bivariate Analysis</strong> - performed to validate or refuse the hypotheses.</li>
									<li><strong>Multivariate Analysis</strong> - performed to check the correlations between the variables. This analysis is performed 
									in different ways. It depends if the variables are numerical or categorical.</li>
								</ul>

								<h3>04.01 - Univariate Analysis</h3>
								
								<h4>04.01.01 - Target Variable</h4>
								<p>It was possible to see in the descriptive statistics that our target variable (<code>sales</code>) has an average of 
								<strong>5,773.82 €</strong> and a median of <strong>5,744.00 €</strong>. It is also possible to see that the sales distribution is not 
								in the center. This variable presents a bell curve, but it is not a normal distribution because it has a moderately skewed distribution 
								(<strong>0.641460</strong>), and its kurtosis is not close to 0 (<strong>1.778375</strong>).</p>
								<p>As most machine learning models require that the data follow a normal distribution, it will be necessary to make the target variable 
								go through some transformations to approach a normal distribution.</p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.01.01_target_variable.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>04.01.02 - Numerical Variables</h4>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.01.02_numerical_variables.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<div class="row">
									<div class="col-6 col-12-small">
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Variable</th>
														<th>Note</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td><code>store</code></td>
														<td>this represents the store's ID, no relevant information to the analysis</td>
													</tr>
													<tr>
														<td><code>day_of_week</code></td>
														<td>the data is uniform; it shows that this variable alone can not explain the target variable</td>
													</tr>
													<tr>
														<td><code>open</code></td>
														<td>this variable was filtered to show only data on opened days</td>
													</tr>
													<tr>
														<td><code>promo</code></td>
														<td>the majority of the observations shows that the stores did not join promotions</td>
													</tr>
													<tr>
														<td><code>school_holiday</code></td>
														<td>regular days are the majority of the observations</td>
													</tr>
													<tr>
														<td><code>competition_distance</code></td>
														<td>there are more observations with stores that have close competitors</td>
													</tr>
													<tr>
														<td><code>competition_open_since_month</code></td>
														<td>September is the month that more competitors are opened, followed by April</td>
													</tr>
													<tr>
														<td><code>competition_open_since_year</code></td>
														<td>the majority of the competitors are opened between 2010 and 2015</td>
													</tr>
													<tr>
														<td><code>promo2</code></td>
														<td>the number of observations of stores that joined consecutive promotion is practically equal to the stores that did not</td>
													</tr>
													<tr>
														<td><code>promo2_since_week</code></td>
														<td>the majority of stores joined consecutive promotion period during the 13th and 14th weeks of the year</td>
													</tr>
													<tr>
														<td><code>promo2_since_year</code></td>
														<td>the majority of stores joined the consecutive promotion in 2013, and the number of stores that join this promotion is in a downtrend</td>
													</tr>
													<tr>
														<td><code>is_promo</code></td>
														<td>the majority of the observations show that stores did not join the promotion period</td>
													</tr>
												</tbody>
											</table>
										</div>
									</div>
									<div class="col-6 col-12-small">
									</div>
								</div>

								<h4>04.01.03 - Categorical Variables</h4>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.01.03_categorical_variables.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<p>When the categorical variables are analyzed, it is possible to see that:</p>
								<ul>
									<li>there is a high spike in sales on Christmas holidays</li>
									<li>type d stores do not have the highest sales but have the highest sales peaks</li>
									<li>the sales volume of stores with extra assortment is more evenly distributed.</li>
								</ul>

								<h3>04.02 - Bivariate Analysis</h3>
								<p>In this part of the project, the hypotheses generated in the previous session will be validated or refused.</p>

								<h4>Hypothesis 1 - Stores with a larger assortment should sell more.</h4>
								<p>In Kaggle's competition, there is no information on what the name of the assortment means. In this project, the assortment type 
								<strong>basic</strong> is interpreted as the small type of assortment that a store has, while the type <strong>extra</strong> is large.</p>
								<p>The chart below shows that stores with <strong>extra</strong> assortment type have the lowest volume of sales.</p>
								<p><strong>Hypothesis 1 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h1.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 2 - Stores with closer competitors should sell less.</h4>
								<p>Below it is possible to see that stores with closer competitors present higher sales. When Pearson's correlation is analyzed, it 
								is seen that there is a week negative correlation (<strong>-0.26</strong>). This shows that the more distant the competitor is, the 
								lower the sales.</p>
								<p><strong>Hypothesis 2 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h2.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 3 - Stores with longer competitors should sell more.</h4>
								<p>The charts below show that the volume of sell increases in periods before the competitions start. After that, when competition 
								begins, the volume of sales decreases, which makes this hypothesis false. It also can be seen in the regression chart. The Person's 
								correlation shows a very weak negative correlation (<strong>-0.1</strong>), and it also confirms that the longer the competition 
								exists, the lower the volume of sales.</p>
								<p><strong>Hypothesis 3 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h3.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 4 - Stores with active promotions for longer should sell more.</h4>
								<p>Below, the first bar chart shows periods of extended promo. This chart shows that the longer the promotion period, the lower 
								the volume of sales. The Pearson's correlation shows a very week negative correlation (<strong>-0.029</strong>).</p>
								<p><strong>Hypothesis 4 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h4.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 5 - Stores with more days of promotion should sell more.</h4>
								<p>Future Analysis</p>

								<h4>Hypothesis 6 - Stores with more consecutive promotions should sell more.</h4>
								<p>A data frame was created to validate this hypothesis. On this data frame is possible to see that 
								stores with more consecutive promotions (<code>promo</code> and <code>promo2</code>) sell less than stores without any promotion 
								period or stores that only joined the regular promotion period. Because of this, this hypothesis is false.</p>
								<p><strong>Hypothesis 6 is refused</strong></p>
								<pre><code>
df4[['promo', 'promo2', 'sales']].groupby(['promo', 'promo2']).sum().reset_index()
								</code></pre>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h6.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 7 - Stores open during the Christmas holiday should sell more.</h4>
								<p>Below it is possible to see that stores sell less during the Christmas Holidays.</p>
								<p><strong>Hypothesis 7 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h7.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 8 - Stores should sell more over the years.</h4>
								<p>Below it is possible to see that the stores are selling less over the years. The Person's correlation shows a strong negative 
								correlation (<strong>-0.92</strong>) between these variables.</p>
								<p><strong>Hypothesis 8 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h8.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 9 - Stores should sell more in the second half of the year.</h4>
								<p>Below it is possible to see that the stores sell less in the second semester of the years. The Person's correlation shows a 
								strong negative correlation (<strong>-0.75</strong>) between these variables.</p>
								<p><strong>Hypothesis 9 is refused</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h9.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 10 - Stores should sell more after the 15th day of each month.</h4>
								<p>Below it is possible to see that the stores sell more after the 15th day of the month. The Person's correlation shows a week 
								negative correlation (<strong>-0.35</strong>) between these variables.</p>
								<p><strong>Hypothesis 10 is validated</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h10.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								
								<h4>Hypothesis 11 - Stores should sell less on weekends.</h4>
								<p>Below it is possible to see that the stores sell less during the weekends. The Person's correlation shows a strong negative 
								correlation (<strong>-0.76</strong>) between these variables.</p>
								<p><strong>Hypothesis 11 is validated</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h11.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypothesis 12 - Stores should sell less during school holidays.</h4>
								<p>In Germany, school holidays have different dates according to the federal State. But usually, the period of school holidays is 
								from middle July until the final of August. Based on this information, the chart below shows that the stores sell more during the 
								school holidays.</p>
								<p><strong>Hypothesis 12 is validated</strong></p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.02_h12.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>Hypotheses Summary</h4>
								<p>Below there is a final summary with the conclusion and its relevance to the machine learning model.</p>
								<div class="row">
									<div class="col-6 col-12-small">
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Hypothesis Number</th>
														<th>Validated / Refused</th>
														<th>Relevance</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>Hypothesis 1</td>
														<td>Refused</td>
														<td>Low</td>
													</tr>
													<tr>
														<td>Hypothesis 2</td>
														<td>Refused</td>
														<td>Medium</td>
													</tr>
													<tr>
														<td>Hypothesis 3</td>
														<td>Refused</td>
														<td>Medium</td>
													</tr>
													<tr>
														<td>Hypothesis 4</td>
														<td>Refused</td>
														<td>Low</td>
													</tr>
													<tr>
														<td>Hypothesis 5</td>
														<td>-</td>
														<td>-</td>
													</tr>
													<tr>
														<td>Hypothesis 6</td>
														<td>Refused</td>
														<td>Low</td>
													</tr>
													<tr>
														<td>Hypothesis 7</td>
														<td>Refused</td>
														<td>Medium</td>
													</tr>
													<tr>
														<td>Hypothesis 8</td>
														<td>Refused</td>
														<td>High</td>
													</tr>
													<tr>
														<td>Hypothesis 9</td>
														<td>Refused</td>
														<td>High</td>
													</tr>
													<tr>
														<td>Hypothesis 10</td>
														<td>Validated</td>
														<td>High</td>
													</tr>
													<tr>
														<td>Hypothesis 11</td>
														<td>Validated</td>
														<td>High</td>
													</tr>
													<tr>
														<td>Hypothesis 12</td>
														<td>Validated</td>
														<td>Low</td>
													</tr>
												</tbody>
											</table>
										</div>
									</div>
									<div class="col-6 col-12-small">
									</div>
								</div>

								<h3>04.03 - Multivariate Analysis</h3>
								<p>After the validation of the hypotheses, it is time to check the correlations between the independent variables. First, the 
								numerical attributes using the method of <strong>Pearson</strong> and then the categorical attributes using the <strong>Cramer V</strong>.</p>

								<h4>04.03.01 - Correlation of Numerical Attibutes</h4>
								<img width="50%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.03_num_att_corr.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<div class="row">
									<div class="col-6 col-12-small">
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Variable A</th>
														<th>Variable B</th>
														<th>Correlation</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td><code>day_of_week</code></td>
														<td><code>school_holiday</code></td>
														<td>Weakly Negative Correlation</td>
													</tr>
													<tr>
														<td><code>day_of_week</code></td>
														<td><code>promo</code></td>
														<td>Weakly Negative Correlation</td>
													</tr>
													<tr>
														<td><code>day_of_week</code></td>
														<td><code>open</code></td>
														<td>Moderately Negative Correlation</td>
													</tr>
													<tr>
														<td><code>day_of_week</code></td>
														<td><code>customers</code></td>
														<td>Weakly Negative Correlation</td>
													</tr>
													<tr>
														<td><code>sales</code></td>
														<td><code>promo</code></td>
														<td>Weakly Negative Correlation</td>
													</tr>
													<tr>
														<td><code>sales</code></td>
														<td><code>open</code></td>
														<td>Moderately Positive Correlation</td>
													</tr>
													<tr>
														<td><code>sales</code></td>
														<td><code>customers</code></td>
														<td>Strongly Positive Correlation</td>
													</tr>
													<tr>
														<td><code>customers</code></td>
														<td><code>promo</code></td>
														<td>Weakly Positive Correlation</td>
													</tr>
													<tr>
														<td><code>customers</code></td>
														<td><code>open</code></td>
														<td>Moderately Positive Correlation</td>
													</tr>
													<tr>
														<td><code>open</code></td>
														<td><code>promo</code></td>
														<td>Weakly Positive Correlation</td>
													</tr>
													<tr>
														<td><code>promo2</code></td>
														<td><code>is_promo</code></td>
														<td>Weakly Positive Correlation</td>
													</tr>
													<tr>
														<td><code>promo2</code></td>
														<td><code>promo2_since_year</code></td>
														<td>Moderately Negative Correlation</td>
													</tr>
													<tr>
														<td><code>promo2_since_year</code></td>
														<td><code>is_promo</code></td>
														<td>Weakly Negative Correlation</td>
													</tr>
												</tbody>
											</table>
										</div>
									</div>
									<div class="col-6 col-12-small">
									</div>
								</div>
								
								<h4>04.03.02 - Correlation of Categorical Attibutes</h4>
								<img width="50%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/04.03_cat_att_corr.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<div class="row">
									<div class="col-6 col-12-small">
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Variable A</th>
														<th>Variable B</th>
														<th>Correlation</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td><code>store_type</code></td>
														<td><code>assortment</code></td>
														<td>Moderately Positive Correlation</td>
													</tr>
												</tbody>
											</table>
										</div>
									</div>
									<div class="col-6 col-12-small">
									</div>
								</div>

								<hr class="major" />

								<h2 id="title"><a name="05_data_preprocessing">05. Data Preprocessing</a></h2>
								<div class="subtitle">
									<a href="#06_feature_selection">(next section)</a> |
									<a href="#04_exploratory_data_analysis">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>In this session of the project, the data preparation for the machine learning models begins.</p>
								<p>Usually, three different methods are applied to the data:</p>
								<ul>
									<li><strong>Normalization</strong>: used in data that has a normal distribution.</li>
									<li><strong>Rescaling</strong>: usually, this method is applied to variables that do not have a normal distribution. The two 
									main rescaling techniques are:</li>
									<ul>
										<li><strong>Min-Max Scaler</strong>: applied using the maximum and minimum values of the data and scales the data in a 
										interval from 0 to 1.</li>
										<li><strong>Robust Scaler</strong>: applied using the interquartile range of the data.</li>
									</ul>
									<li><strong>Transformation</strong>: data are prepared using methods of encoding, magnitude, or nature.</li>
								</ul>

								<h3>05.01 - Normalization</h3>
								<p>When the numerical data distributions were analyzed, it was possible to see that the variables do not follow a normal 
								distribution. For this reason, I will not apply this type of transformation to this project.</p>

								<h3>05.02 - Rescaling</h3>
								<p>Here both methods (<strong>Min-Max Scaler</strong> and <strong>Robust Scaler</strong>) were applied. The 
								<strong>Min-Max Scaler</strong> was applied in the variables <code>year</code> and <code>promo_time_week</code>, and it is possible 
								to see that the values now are distributed in an interval from 0 to 1. The <strong>Robust Scaler</strong> was applied in the variables 
								<code>competition_distance</code> and <code>promo_time_week</code>, and it is possible that the values now are distributed based on 
								the values from the interquartile range of the variables.</p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.02_boxplot_rescaling.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h3>05.03 - Transformation</h3>
								<p>In this session, the methods of <strong>encoding, magnitude, and nature</strong> were used to transform the variables. The 
								encoding methods were used to transform the categorical variables, the magnitude method to transform the target variable, and 
								the nature method to transform the cyclic variables.</p>

								<h4>05.03.01 - Encoding</h4>
								<p><strong>One Hot Encoding</strong>, variable <code>state_holiday</code></p>
								<div class="table-wrapper">
									<table>
										<thead>
											<tr>
												<th style="text-align: center;">Before Encoding</th>
												<th style="text-align: center;">After Encoding</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.01_state_holiday_original.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.01_state_holiday_encoded.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
											</tr>
										</tbody>
									</table>
								</div>
								
								<p><strong>Label Encoding</strong>, variable <code>store_type</code></p>
								<div class="table-wrapper">
									<table>
										<thead>
											<tr>
												<th style="text-align: center;">Before Encoding</th>
												<th style="text-align: center;">After Encoding</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.01_store_type_original.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.01_store_type_encoded.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
											</tr>
										</tbody>
									</table>
								</div>

								<p><strong>Ordinal Encoding</strong>, variable <code>assortment</code></p>
								<div class="table-wrapper">
									<table>
										<thead>
											<tr>
												<th style="text-align: center;">Before Encoding</th>
												<th style="text-align: center;">After Encoding</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.01_assortment_original.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.01_assortment_encoded.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
											</tr>
										</tbody>
									</table>
								</div>

								<h4>05.03.02 - Magnitude</h4>
								<p>Application of logarithm transformation on the target variable, <code>sales</code>.</p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.02_target_variable_magnitude_transformation.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h4>05.03.03 - Nature</h4>
								<p>In the variables that present cyclic periods as <code>day</code>, <code>month</code>, <code>week_of_year</code> and <code>day_of_week</code>, 
								the nature transformation was performed. Below the result:</p>
								<div class="table-wrapper">
									<table>
										<thead>
											<tr>
												<th style="text-align: center;">Before Nature Transformation</th>
												<th style="text-align: center;">After Nature Transformation</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.03_cyclic_variables_original.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
												<td><img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/05.03.03_cyclic_variables_after.png" style="display:block; margin-left: auto; margin-right: auto;"></td>
											</tr>
										</tbody>
									</table>
								</div>

								<hr class="major" />

								<h2 id="title"><a name="06_feature_selection">06. Feature Selection</a></h2>
								<div class="subtitle">
									<a href="#07_machine_learning_modeling">(next section)</a> |
									<a href="#05_data_preprocessing">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>In this session of the project, a <strong>Wrapper Method</strong> of feature selection was applied. A Wrapper Method is a method 
								that uses an algorithm to select the best features on the dataset. The algorithm used in this project is the <strong>Boruta</strong>.</p>
								<p>Below is the code to run the Boruta through the train and test datasets:</p>
								<pre><code>
# training and test dataset for boruta

X_train_n = X_train.drop(['date', 'sales'], axis = 1).values

y_train_n = y_train.values.ravel()

# define random forest regressor

rf = RandomForestRegressor(n_jobs = -1)

# define boruta

boruta = BorutaPy(rf, n_estimators = 'auto', verbose = 2, random_state = 42).fit(X_train_n, y_train_n)
								</code></pre>
								<p>It is possible that the Boruta made its selection of variables, but the Boruta did not select two variables as relevant: 
								<code>month_sin</code> and <code>week_of_year_sin</code>. As the variables <code>month_cos</code> and <code>week_of_year_cos</code> 
								were selected by the algorithm, I will include these two variables not selected by the Boruta as relevant.</p>
								<pre><code>
cols_selected_boruta

    ['store',
     'promo',
     'store_type',
     'assortment',
     'competition_distance',
     'competition_open_since_month',
     'competition_open_since_year',
     'promo2',
     'promo2_since_week',
     'promo2_since_year',
     'competition_time_month',
     'promo_time_week',
     'day_of_week_sin',
     'day_of_week_cos',
     'month_cos',
     'day_sin',
     'day_cos',
     'week_of_year_cos']
								</code></pre>
								<p>Before running the algorithm, two variables were excluded: <code>date</code> and <code>sales</code>. These two variables need to 
								be used for the machine learning model, so these two variables will also be returning to the list of relevant variables.</p>
								<p>Therefore, these four variables are added manually to the final list of variables, and the list of variables selected is the following:</p>
								<pre><code>
cols_selected_boruta = ['store',
                        'promo',
                        'store_type',
                        'assortment',
                        'competition_distance',
                        'competition_open_since_month',
                        'competition_open_since_year',
                        'promo2',
                        'promo2_since_week',
                        'promo2_since_year',
                        'competition_time_month',
                        'promo_time_week',
                        'month_cos',
                        'day_of_week_sin',
                        'day_of_week_cos',
                        'day_sin',
                        'day_cos',
                        'week_of_year_cos']
                        
features_to_add = ['week_of_year_sin', 'month_sin', 'date', 'sales']
								</code></pre>

								<hr class="major" />

								<h2 id="title"><a name="07_machine_learning_modeling">07. Machine Learning Modeling</a></h2>
								<div class="subtitle">
									<a href="#08_hyperparameter_tuning">(next section)</a> |
									<a href="#06_feature_selection">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>In this session, seven models will be applied in the dataset of train: <strong>Average Model</strong>, 
								<strong>Linear Regression</strong>, <strong>Lasso Regression</strong>, <strong>Random Forest Regression</strong>, and 
								<strong>XGBoost Regression</strong>. The <strong>Average Model</strong> will be a baseline. With this model, it will be possible 
								to compare the performance of the other models and check if they are performing better than the simple average.</p>
								<p>To avoid overfitting and check how much the model generalizes, a Cross-Validation technique will be applied. A comparison 
								between the results will be made at the final of this session, and a final model chose for the hyperparameter tuning and deployment.</p>

								<h3>07.01 - Models Single Performance</h3>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/07.01_models_single_performance.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h3>07.02 - Models with Cross-Validation Performance</h3>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/07.03_models_crossvalidation_performance.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h3>07.03 - Chosen Model</h3>
								<p>After the Cross-Validation, it is possible to see that the <strong>Random Forest Regressor</strong> presented the lowest RMSE 
								value (<strong>1256.27 +/- 319.16</strong>). However, in this project the <strong>XGBoost</strong> will be chosen to continue with 
								the hyperparameter tuning and deployment.</p>

								<hr class="major" />

								<h2 id="title"><a name="08_hyperparameter_tuning">08. Hyperparameter Tuning</a></h2>
								<div class="subtitle">
									<a href="#09_understaing_the_error_and_business_performance">(next section)</a> |
									<a href="#07_machine_learning_modeling">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>
								<p>In this session, the model will pass through a process of Hyperparameter Tuning. This process seeks to find the best parameters 
								for the model. The main methods to search for the best model parameters are <strong>Random Search</strong>, 
								<strong>Grid Search</strong>, and <strong>Bayesian Search</strong>. In this project will be applied Random Search.</p>
								<p>The following code was used to find the hyperparameters, and below there are their results:</p>
								<pre><code>
# parameters dictionary

parameters = {'n_estimators': [1500, 1700, 2500, 3000, 3500],
              'eta': [0.01, 0.03],
              'max_depth': [3, 5, 9],
              'subsample': [0.1, 0.5, 0.7],
              'colsample_bytree': [0.3, 0.7, 0.9],
              'min_child_weight': [3, 8, 15]
             }

MAX_EVAL = 10

final_result = pd.DataFrame()

# loop for random search

for i in range(MAX_EVAL):
    
    # choose values for parameters randomly
    
    hp = {k: random.sample(v, 1)[0] for k, v in parameters.items()}
    print(hp)

    # model
    
    model_xgb = xgb.XGBRegressor(objective = 'reg:squarederror',
                                 n_estimators = hp['n_estimators'],
                                 eta = hp['eta'],
                                 max_depth = hp['max_depth'],
                                 subsample = hp['subsample'],
                                 colsample_bytree = hp['colsample_bytree'], 
                                 min_child_weight = hp['min_child_weight'])
       
    # performance
    
    result = cross_validation('XGBoost', model_xgb, x_training, 2, verbose = False)
    final_result = pd.concat([final_result, result])
    
final_result

{'n_estimators': 2500, 'eta': 0.03, 'max_depth': 5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 15}
{'n_estimators': 1700, 'eta': 0.03, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 8}
{'n_estimators': 3500, 'eta': 0.03, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 0.3, 'min_child_weight': 3}
{'n_estimators': 1500, 'eta': 0.03, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 0.3, 'min_child_weight': 8}
{'n_estimators': 1500, 'eta': 0.03, 'max_depth': 5, 'subsample': 0.1, 'colsample_bytree': 0.9, 'min_child_weight': 15}
{'n_estimators': 3500, 'eta': 0.01, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 15}
{'n_estimators': 3000, 'eta': 0.03, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.3, 'min_child_weight': 3}
{'n_estimators': 2500, 'eta': 0.01, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 0.3, 'min_child_weight': 8}
{'n_estimators': 3500, 'eta': 0.03, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 15}
{'n_estimators': 3000, 'eta': 0.01, 'max_depth': 9, 'subsample': 0.5, 'colsample_bytree': 0.9, 'min_child_weight': 8}
								</code></pre>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/08_hyperparameter_tuning.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<p>After that the best parameters was found (in this project <code>n_estimators = 3500</code>, <code>eta = 0.03</code>, 
								<code>max_depth = 9</code>, <code>subsample = 0.7</code>, <code>colsample_bytree = 0.9</code> and <code>min_child_weight = 15</code>) 
								the model was trained and tested again giving the following result:</p>
								<p><em>P.S - The model used in this project has the parameters</em> <code>n_estimators = 1500</code>, <code>eta = 0.03</code>, 
								<code>max_depth = 9</code>, <code>subsample = 0.7</code>, <code>colsample_bytree = 0.7</code> and <code>min_child_weight = 15</code>
								<em>because the Github does not accept files with more than 100MB.</em></p>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/08_final_model_result.png" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<p>Below there is a comparison between the models before and after the hyperparameter tuning:</p>
								<div class="row">
									<div class="col-6 col-12-small">
										<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th></th>
														<th>MAE</th>
														<th>MAPE</th>
														<th>RMSE</th>
													</tr>
												</thead>
												<tbody>
													<tr>
														<td>Before Hyperparameter Tuning</td>
														<td>7049.09</td>
														<td>0.95</td>
														<td>7715.12</td>
													</tr>
													<tr>
														<td>After Hyperparameter Tuning</td>
														<td>644.43</td>
														<td>0.09</td>
														<td>942.93</td>
													</tr>
												</tbody>
											</table>
										</div>
									</div>
									<div class="col-6 col-12-small">
									</div>
								</div>

								<hr class="major" />

								<h2 id="title"><a name="09_understaing_the_error_and_business_performance">09. Understanding the Error and Business Performance</a></h2>
								<div class="subtitle">
									<a href="#10_model_deployment_and_final_solution">(next section)</a> |
									<a href="#08_hyperparameter_tuning">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>

								<p>This part of the project is one of the essential parts of any data science project. Here, the errors generated by the model are interpreted, 
								and these errors are translated into a business language, so people from other areas can see how viable the model is for the business.</p>

								<h3>09.01 - Business Performance</h3>
								<p>Above, we could see that the model has a MAE of 644.43 and a MAPE of 0.09. However, this value considers the errors of the predictions of all 
								stores. In this project, the MAE and MAPE per store will be analyzed.</p>
								<p>Thus the best and worst scenarios per store will be generated considering the value of the predictions plus the MAPE for the best and the value 
								of the predictions minus the MAPE for the worst scenario. Below, we can see seven stores with a MAPE greater than 20% and the MAE 
								(daily mean absolute error).</p>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/9_business_performance_tab.PNG" style="display:block; margin-left: auto; margin-right: auto;"><br>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/9_business_performance.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<p>When all stores are taken into account, for the next six weeks, the predictions indicate that 285,068,160.00 € will be sold, with the worst-case 
								forecast being 258,561,074.66 € and the best-case forecasting being 311,575,244.74 €.</p>
								<img src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/9_total_performance.PNG" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<h3>09.02 - Model Performance</h3>

								<p>The evaluation of the model is essential before deploying it into production. The figure below shows four graphs for this assessment to be made.</p>

								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/9_machine_learning_performance.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<ul>
									<li>In the first graph (upper left graph), we can see that the predictions (represented by the orange line) follow very close to the real values ​​(blue line).</li>
									<li>The second graph (upper right graph) shows us the error rate: the ratio between predicted  ​​and observed values. The closer these values ​​are to the 
									orange dashed line, the better.</li>
									<li>The third graph (lower left graph) shows us the distribution of errors. A good machine learning model presents errors following a normal distribution. 
									It can be seen in this graph that our errors are following this pattern, where the vast majority of errors are in the center of the distribution.</li>
									<li>The fourth graph (lower right graph) shows us the forecasts plotted against error by store and date. Ideally, this graph should be in the form of a tube, 
									as it shows the variation in the model's error is low. However, it can be seen that in 11 days, the model had errors greater than 10,000 €.</li>
								</ul>

								<hr class="major" />

								<h2 id="title"><a name="10_model_deployment_and_final_solution">10. Model Deployment and Final Solution</a></h2>
								<div class="subtitle">
									<a href="#conclusion">(next section)</a> |
									<a href="#09_understaing_the_error_and_business_performance">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>

								<p>To deploy the model and create the final solution (which will be a bot in the Telegram app that will send to the end-user the prediction 
								of total sales for a given store in 6 weeks), I will use the <a href="https://www.heroku.com" target="_blank">Heroku</a> platform to host 
								both applications (model and Telegram bot).</p>

								<h3>10.01 - Model Deployment</h3>

								<p>The final model will be implemented in an application called <code>gh-rossmann-prediction</code> on Heroku's platform. This entire application is in the 
								<a href="https://github.com/ghermsen/rossmann_prediction/tree/main/webapp_rossmann" target="_blank">webapp_rossmann</a> folder in this project's repository.</p>
								<p>In this folder, I created a file called <code>handler.py</code>. This file will send the data to a file called <code>Rossmann.py</code> that will prepare the 
								data to then be sent to the model to perform the prediction. This <code>handler.py</code> file will connect to the bot that will go into another application on Heroku.</p>

								<h3>10.02 - Telegram Bot</h3>

								<p>The bot in the Telegram app was created through BotFather (<a href="https://core.telegram.org/" target="_blank">see the documentation</a>), and the entire app is hosted 
								in the <code>api_telegram_rossmann</code> folder. There is a file called <code>rossmann-bot.py</code> that will receive the user's message via Telegram, and this file connects 
								to the <code>handler.py</code> file to receive and send the prediction generated by the model to the user.</p>

								<h3>10.03 - Project's Architecture and Final Solution</h3>

								<p>Below there is an image illustrating the project's architecture.</p>
								<img width="70%" src="https://raw.githubusercontent.com/ghermsen/rossmann_prediction/main/img/10_project_architecture.png" style="display:block; margin-left: auto; margin-right: auto;"><br>

								<p>And below, there is a demonstration of the final solution, the telegram robot connecting with the trained model, receiving the predictions, and sending them to the user.</p>
								<img src="https://github.com/ghermsen/rossmann_prediction/blob/main/img/10_final_solution.gif" style="display:block; margin-left: auto; margin-right: auto;"><br>



								<hr class="major" />

								<h2 id="title"><a name="conclusion">Conclusion</a></h2>
								<div class="subtitle">
									<a href="#10_model_deployment_and_final_solution">(previous section)</a> | 
									<a href="#table_of_contents">Table of Contents</a>
								</div>

								<p>This project addressed the first cycle of a data science project using CRISP-DM as a methodology. I can conclude that this first step brought the expected result for the 
								end-user, as it can be seen that the model has a good performance. However, some improvements can be made in future cycles, such as:</p>
								<ul>
									<li>create a model to predict customers demand so that the variable number of customers can be used in the sales forecast.</li>
									<li>train other machine learning models to see if there is an improvement in the predictions, especially in stores 292, 909, 876, 722, 595, 274, and 782. If these stores do 
									not show significant improvements, it is ideal for conducting a more detailed analysis to generate better predictions.</li>
									<li>creation of new variables and see how much impact the RMSE is with these new variables</li>
								</ul>
								<p>I want to thank you for taking the time to read this report so far and say that suggestions and corrections for this project are highly welcome.</p>

							</section>

							<!-- Contact Footer -->
							<section w3-include-html="/gabrielhermsen/components/footer.html">
							</section>

						</div>
					</div>
			</div>

		<!-- Scripts -->
			<script src="/gabrielhermsen/assets/js/jquery.min.js"></script>
			<script src="/gabrielhermsen/assets/js/browser.min.js"></script>
			<script src="/gabrielhermsen/assets/js/breakpoints.min.js"></script>
			<script src="/gabrielhermsen/assets/js/util.js"></script>
			<script src="/gabrielhermsen/assets/js/main.js"></script>
			<script src="/gabrielhermsen/assets/js/w3.js"></script>
			<script>w3.includeHTML();</script>

	</body>
</html>
